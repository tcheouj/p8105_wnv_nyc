---
title: "exploratory_data_analysis"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(httr)
library(GSODR)
library(sf)
library(patchwork)
```


```{r wnv data import, echo = FALSE}
zip_nbhd <- read_html("https://p8105.com/data/zip_codes.html") |>
  html_table() |>
  first() |>
  janitor::clean_names() |>
  filter(!(county == "New York" & zip_code %in% c(10463, 11201))) |>#duplicated zip codes, both are *not* in New York County
  mutate(borough = str_replace_all(county, c(
    "Kings" = "Brooklyn",
    "New York" = "Manhattan",
    "Richmond" = "Staten Island"
  ))) |>
  select(zip_code, borough, neighborhood)

## 2021 - 2023 are simple HTML tables
mosquitoes_2024_url <- "https://www.nyc.gov/site/doh/health/health-topics/west-nile-virus-activity-2024.page"
mosquitoes_2023_url <- "https://www.nyc.gov/site/doh/health/health-topics/west-nile-virus-activity-2023.page"
mosquitoes_2022_url <- "https://www.nyc.gov/site/doh/health/health-topics/west-nile-virus-activity-2022.page"
mosquitoes_2021_url <- "https://www.nyc.gov/site/doh/health/health-topics/west-nile-virus-activity-2021.page"

### 2024
mosquitoes_2024_table <- read_csv("data/mosquitoes_2024.csv") |>
  pivot_longer(cols = starts_with("date"), values_to = "date") |>
  select(-name, -detection_type) |>  
  mutate(
    date = case_when(
      date == "8/5/5024" ~ "8/5/2024",
      .default = date # one observation, 11426, with year miswritten as 5024-08-05 instead of 2024-08-05
    ),
    date = lubridate::mdy(date)
  ) |> 
  na.omit() 
   
### 2023
mosquitoes_2023_table <- mosquitoes_2023_url |>
  read_html() |> html_table() |> first() |>
  janitor::clean_names() |>
  separate_longer_delim(zip_code, delim = ", ") |>
  separate_longer_delim(zip_code, delim = ",\n") |>
  separate_longer_delim(zip_code, delim = ",") |> #this data is disgusting
  select(-neighborhood, -download) |>
  mutate(
    zip_code = as.integer(zip_code),
    date = str_c(date, ", ", 2023),
    date = lubridate::mdy(date)
  )

### 2022
mosquitoes_2022_table <- mosquitoes_2022_url |>
  read_html() |> html_table() |> first() |>
  janitor::clean_names() |>
  separate_longer_delim(zip_code, delim = ", ") |>
  separate_longer_delim(zip_code, delim = ",\n") |>
  separate_longer_delim(zip_code, delim = ",") |> #this data is disgusting
  select(-download) |>
  mutate(
    zip_code = as.integer(zip_code),
    date = str_c(date, ", ", 2022),
    date = lubridate::mdy(date)
  )

### 2021
mosquitoes_2021_table <- mosquitoes_2021_url |>
  read_html() |> html_table() |> first() |>
  janitor::clean_names() |>
  separate_longer_delim(zip_code, delim = ", ") |>
  separate_longer_delim(zip_code, delim = ",\n") |>
  separate_longer_delim(zip_code, delim = ",") |> #this data is disgusting
  select(-download) |>
  mutate(
    zip_code = as.integer(zip_code),
    date = str_c(date, ", ", 2021),
    date = lubridate::mdy(date)
  ) |> 
  mutate(
    zip_code = case_when(
      zip_code == 1123 ~ 11236, # data entry error on the website, where 1123 is missing the last digit - per https://www.nyc.gov/assets/doh/downloads/pdf/wnv/2021/wnv-activity-bk-08192021.pdf it should be 11236
    .default = zip_code
    )
  )

wnv_mosquito_detection <- bind_rows(
  mosquitoes_2024_table,
  mosquitoes_2023_table,
  mosquitoes_2022_table,
  mosquitoes_2021_table,
) |> 
  left_join(y = zip_nbhd, by = join_by(zip_code)) |>
  select(zip_code, borough.x, date, neighborhood) |> 
  rename(borough = borough.x) |> # one ZIP code, 10000 has NA for borough as it is not in ZIP list - Central Park based on a Google search
  mutate(
    borough = case_when(
      zip_code == 10000 ~ "Manhattan",
      .default = borough
    ),
    neighborhood = case_when(
      zip_code == 10000 ~ "Central Park",
      .default = neighborhood
    )
  )

######### Heat Vulnerability by Neighborhood
heat_vuln <- read_csv("data/heat_vulnerability.csv") |>
  janitor::clean_names() #Already clean! Thank you NYC environment and health

######### West Nile Virus Cases by Year and Borough
wnv_cases <- read_csv("data/wnv_cases.csv") |>
  janitor::clean_names() |>
  pivot_longer(cols = c(
    bronx,
    brooklyn,
    manhattan,
    queens,
    staten_island
  ),
  names_to = "borough",
  values_to = "wnv_cases") |> 
  mutate(
    borough = case_match(
      borough, 
      "manhattan" ~ "Manhattan",
      "brooklyn" ~ "Brooklyn",
      "queens" ~ "Queens",
      "bronx" ~ "Bronx",
      "staten_island" ~ "Staten Island"
    )
  )

# May need to use ZCTA level HVI data instead of NTA level HVI 
# - ZCTA is designed to approximate ZIP codes, when our WNV counts are by ZIP code

hvi_zcta <- read_csv("https://data.cityofnewyork.us/resource/4mhf-duep.csv")  |> 
  rename(zip_code = zcta20)

inner_join(wnv_mosquito_detection, hvi_zcta, by = join_by(zip_code)) |> 
  group_by(hvi) |> 
  summarize(
    count = n()
  )
```

```{r add no wnv zctas}
# get ZIP codes that did not have counts of WNV using anti_join with original ZIP code list (for generating proportions and to ensure ZIPs without cases can still be mapped later)


all_zctas <-
  zip_nbhd |> 
  pull(zip_code) |> 
  c(10000) |> 
  expand_grid(
    zip_code = _,
    year = c(2021, 2022, 2023, 2024),
    count = 0
  )

fill_neg_zctas <- function(yr) {
  
  pos = 
    wnv_mosquito_detection |> 
    mutate(
      year = year(date)
    ) |> 
    filter(year == yr) |> 
    group_by(zip_code, neighborhood, year) |> 
    summarize(
      count = n()
    ) |>
    arrange(count)
  
  neg = 
    all_zctas |> 
    filter(year == yr) |> 
    anti_join(pos, by = join_by(zip_code))
  
  rbind(pos, neg)
    
}

wnv_mosquito_data <-
  tibble(
    years = c(2021, 2022, 2023, 2024),
    mosquito_data = map(years, fill_neg_zctas)
  ) |> 
  unnest(mosquito_data) |> 
  select(-years) |> 
  left_join(zip_nbhd, by = join_by(zip_code, neighborhood)) |> # rejoin borough variable from original ZIP code list 
  mutate(
    borough = case_when(
      zip_code == 10000 ~ "Manhattan",
      .default = borough
    ),
    neighborhood = case_when(
      zip_code == 10000 ~ "Central Park",
      .default = neighborhood
    )
  )

wnv_mosquito_data |> 
  pull(count) |> 
  sum() # 2924

wnv_mosquito_detection |> 
  nrow() # 2924 - matches

```


```{r gsodr data import, echo = FALSE}

# Bushwick is the geographical center of NYC, providing the following latitude and longitude coordinates 
# Wakefield at the top of the Bronx is around 33 km/20.5 mi away from Bushwick, creating the radius of stations to search for
# Filter for non NYC stations, by filtering out NJ and filtering out Kings Point and Sandy Hook and other Long Island locations

station_ids <-
  nearest_stations(
    LAT = 40.6958, 
    LON = -73.9171,
    distance = 20
  ) |> 
  filter(STATE != "NJ" & !(NAME %in% c("KINGS POINT", 
                                       "SANDY HOOK", 
                                       "BERGEN POINT", 
                                       "MITCHEL FIELD", 
                                       "HEMPSTEAD MITCHELL FLD AFB", 
                                       "AMBROSE LIGHT  NY", 
                                       "AMBROSE / FT. TILDEN", 
                                       "FREEPORT"))) |> 
  filter(!(STNID %in% c("999999-14732",
                        "725033-94728",
                        "725060-94728",
                        "999999-94728",
                        "744976-99999",
                        "999999-14786",
                        "999999-94789",
                        "725026-99999",
                        "997439-99999"))) |> # Some stations only have historical data preceding our years of interest of 2021-2024, these have been filtered out
  janitor::clean_names() |> 
  pull(stnid)

# weather_2021_2024 <-
#   get_GSOD(
#     years = c(2021:2024),
#     station = station_ids
#   ) |> 
#   janitor::clean_names() |>
#   select(name, latitude, longitude, yearmoda, temp, max, min, prcp, dewp) |>  # retain relevant variables - station name, latitude and longitude, date (yearmoda) mean temperature (temp), mean precipitation (prcp), mean dew point (dewp) to get an idea of water content in the air, max temperature, and min temperature
#   mutate(
#     temp = (temp * 9/5) + 32, # convert from Celsius to Fahrenheit
#     max = (max * 9/5) + 32,
#     min = (min * 9/5) + 32,
#     prcp = (prcp * 0.0393701), # convert from mm to in
#     year = year(yearmoda), # give stations more intuitive names
#     name = case_match(
#       name, 
#       "PORT AUTH DOWNTN MANHATTAN WALL ST HEL" ~ "Wall St",
#       "LA GUARDIA AIRPORT" ~ "LGA",
#       "CENTRAL PARK" ~ "Central Park",
#       "JOHN F KENNEDY INTERNATIONAL AIRPORT" ~ "JFK",
#       "THE BATTERY" ~ "The Battery"
#     )
#   )

# weather_1999_2023 <-
#   get_GSOD(
#     years = c(1999:2023),
#     station = c("725030-14732", "720553-99999", "744860-94789")
#   ) |> 
#   janitor::clean_names() |>
#   select(name, latitude, longitude, yearmoda, temp, max, min, prcp, dewp) |>  # retain relevant variables - station name, latitude and longitude, date (yearmoda) mean temperature (temp), mean precipitation (prcp), mean dew point (dewp) to get an idea of water content in the air, max temperature, and min temperature
#   mutate(
#     temp = (temp * 9/5) + 32, # convert from Celsius to Fahrenheit
#     max = (max * 9/5) + 32,
#     min = (min * 9/5) + 32,
#     prcp = (prcp * 0.0393701), # convert from mm to in
#     year = year(yearmoda) # give stations more intuitive names
#   ) |> 
#   rename(date = yearmoda)
# # 725030-14732 - LGA
# # 725053-94728 - Central Park - only data from 2005-2024
# # 744860-94789 - JFK 
# # 720553-99999 - Port Authority Downtown Helipad - only data from 2016-2024
# 
# 
# yearly_weather <-
#   weather_1999_2023 |> 
#   mutate(
#     year = year(date)
#   ) |> 
#   group_by(year) |> 
#   summarize(
#     avg_temp = mean(temp),
#     avg_min = mean(min),
#     avg_max = mean(max), 
#     avg_prcp = mean(prcp, na.rm = TRUE),
#     avg_dewp = mean(dewp)
#   )
# 
# day_weather <-
#   weather_1999_2023 |> 
#   group_by(date) |> 
#   summarize(
#     avg_temp = mean(temp),
#     avg_min = mean(min),
#     avg_max = mean(max), 
#     avg_prcp = mean(prcp, na.rm = TRUE),
#     avg_dewp = mean(dewp)
#   )

#write_csv(yearly_weather, "cleaned_data/yearly_weather.csv")
#write_csv(day_weather, "cleaned_data/day_weather.csv")

yearly_weather <- read_csv("cleaned_data/yearly_weather.csv")

day_weather <- read_csv("cleaned_data/day_weather.csv")


```

``` {r cursory graphs}
# Mosquito counts by year and borough - does mosquito season start earlier?

mosquitoes_2021_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  facet_grid(~borough)

mosquitoes_2022_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |>   
  ggplot(aes(x = date, y = count)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  facet_grid(~borough)


mosquitoes_2023_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  facet_grid(~borough)

mosquitoes_2024_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  facet_grid(~borough)

mosquitoes_2021_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count, fill = borough)) +
  geom_col(stat = "identity") +
  facet_grid(~borough)

mosquitoes_2022_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count, fill = borough)) +
  geom_col(stat = "identity") +
  facet_grid(~borough)

mosquitoes_2023_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count, fill = borough)) +
  geom_col(stat = "identity") +
  facet_grid(~borough)

mosquitoes_2024_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count, fill = borough)) +
  geom_col(stat = "identity") +
  facet_grid(~borough)

wnv_mosquito_detection |> 
  mutate(count = 1) |> 
  group_by(date) |> 
  summarize(
    count = sum(count)
  ) |> 
  ggplot(aes(x = date, y = count)) +
  geom_point()

```


```{r geocoding mosquito data by zcta, echo=FALSE}
# get geographic coordinates for use in ggplot later

modzcta_path <- "data/shapefiles/MODZCTA_2010.shp"

zip_shp <-
  modzcta_path |>
  st_read() |> 
  drop_na(label) |> # gets rid of MODZCTA 99999 which is not a real MODZCTA 
  separate_longer_delim(label, delim = ", ") |> 
  janitor::clean_names() |> 
  mutate(
    modzcta = as.double(modzcta),
    zip_code = as.double(label)
  ) |> 
  separate_longer_delim(label, delim = ", ")

# get MODZCTA for each ZIP code (approximating ZIP code as ZCTA)

mos_by_modzcta <- 
  left_join(wnv_mosquito_data, zip_shp, by = join_by("zip_code")) |> 
  mutate(
    neighborhood = case_when(
      is.na(neighborhood) ~ "Non-residential",
      .default = neighborhood
    )
  )
```

```{r geocoding human case data, echo=FALSE}

boro_path <- "data/shapefiles/nybb.shp"

boro_shp <-
  boro_path |>
  st_read() |> 
  janitor::clean_names() |> 
  rename(borough = boro_name) |> 
  select(borough, geometry)
  
# merge case data with borough shapefiles

wnv_cases_shp <-
  left_join(wnv_cases, boro_shp, by = join_by(borough))

wnv_cases_shp |> 
  ggplot(aes(fill = wnv_cases, geometry = geometry)) + 
  geom_sf() + 
  labs(
    title = "WNV positive human cases by borough",
    caption = "Data and shapefile from DOHMH"
  ) +
  theme(
    axis.text.x = element_blank(), 
    axis.title.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  ) +
  viridis::scale_fill_viridis() +
  facet_wrap(~year)
  
```


```{r map pos mosquito per modzcta}
mos_by_modzcta |>
  ggplot(aes(fill = count, geometry = geometry)) +
  geom_sf() + 
  labs(
    title = "WNV positive mosquitoes by MODZCTA",
    caption = "Data and shapefile from DOHMH"
  ) +
  theme(
    axis.text.x = element_blank(), 
    axis.title.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  ) +
  viridis::scale_fill_viridis(limits = c(0, 25)) +
  facet_wrap(~year)
```

```{r case data}
avg_hvi_borough <-
  left_join(hvi_zcta, zip_nbhd, by = join_by(zip_code)) |> 
    group_by(borough) |> 
    summarize(
      avg_hvi = mean(hvi)
    )

# is heat vulnerability associated with WNV+ human cases?

left_join(wnv_cases, yearly_weather, by = join_by(year)) |> 
  left_join(avg_hvi_borough, by = join_by(borough)) |> 
  ggplot(aes(x = avg_hvi, y = wnv_cases, fill = year, shape = borough)) +
  geom_point() + 
  stat_summary(fun.y = mean, color = "red")  # perhaps hvi is a parabolic distribution?



merged_df <- 
  wnv_mosquito_data |> 
  filter(!is.na(borough)) |>  # there are (likely nonresidential) ZIP codes with no boroughs and no detected WNV in mosquitoes and humans
  group_by(year, borough) |> 
  summarize(
    wnv_mosquitoes = sum(count) 
  ) |> 
  right_join(wnv_cases, by = join_by(year, borough)) |> 
  left_join(yearly_weather, by = join_by(year)) |> 
  left_join(avg_hvi_borough, by = join_by(borough)) |>  
  select(year, borough, starts_with("wnv_"), everything()) |> 
  arrange(year, borough)

```


```{r poisson assumptions}

merged_df |> 
  ggplot(aes(x = wnv_cases)) +
  geom_histogram() +
  facet_grid(~borough) # right tailed

temp_quantiles <-
  merged_df |> 
  pull(avg_temp) |> 
  quantile(probs = c(0.25, 0.50, 0.75))

temp_cat <-
  merged_df |> 
  mutate(
    temp_quantile = case_when(
      avg_temp < temp_quantiles[1] ~ 1,
      avg_temp < temp_quantiles[2] ~ 2,
      avg_temp < temp_quantiles[3] ~ 3,
      avg_temp > temp_quantiles[3] ~ 4,
      .default = NA
    )
  ) # right tailed


temp_cat |> 
  ggplot(aes(x = wnv_cases)) +
  geom_histogram() +
  facet_grid(~temp_quantile) 

  
# check assumption that mean and variance are approximately equal for each group of x (temperature)
# graphically 

merged_df |> 
  ggplot(aes(x = avg_temp, y = wnv_cases)) + 
  geom_point() +
  geom_smooth(se = FALSE)

merged_df |> 
  ggplot(aes(x = avg_temp, y = wnv_cases, color = borough)) + 
  geom_point() +
  geom_smooth(se = FALSE)

merged_df |> 
  ggplot(aes(x = avg_temp, y = wnv_cases, color =)) + 
  geom_point() +
  geom_smooth(se = FALSE)

merged_df |> 
  pull(wnv_cases) |> 
  max() # highest number of human WNV cases is 34 - which happened in 1999, when WNV first reached NYC

merged_df |> 
  filter(wnv_cases == 34)
  
# tabular check 
merged_df |> 
  group_by(avg_temp) |> 
  summarize(
    avg_count = mean(wnv_cases), 
    variance = var(wnv_cases)
  ) |> 
  arrange(desc(avg_count))


# borough vs count
merged_df |> 
  group_by(borough) |> 
  summarize(
    avg_count = mean(wnv_cases), 
    variance = var(wnv_cases)
  ) |> 
  arrange(desc(avg_count)) 

# test if log of wnv count is linearly related to predictors

merged_df |> 
  mutate(
    log_cases = log(wnv_cases)
  ) |> 
  ggplot(aes(x = avg_temp, y = log_cases)) +
  geom_point() +
  geom_smooth( se = FALSE) # not really

merged_df |> 
  mutate(
    log_cases = log(wnv_cases)
  ) |> 
  ggplot(aes(x = avg_temp, y = log_cases, color = borough)) +
  geom_point() +
  geom_smooth( se = FALSE) # not really


```

