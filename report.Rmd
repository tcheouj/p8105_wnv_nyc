---
title: "**Buzzing Threats: Climate Change, Mosquitoes, and West Nile Virus in New York City**"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
library(broom)
library(rvest)
library(httr)
library(GSODR)
library(sf)
library(patchwork)
library(car)
library(GGally)
library(broom.helpers)
```

## **Motivation**

### Inspiration
Our group has a shared interest in addressing climate change, studying infectious diseases, and uniting over our mutual hatred of mosquito bites. Inspiration for this project stems from a few articles: 

* A [2024 New York Times article](https://www.nytimes.com/article/west-nile-virus-nyc.html) highlights the rising concern for an increased number of mosquitoes infected with West Nile virus (WNV) over the summer in New York City (NYC) and how this likely driven by climate change. 
* A [2020 Nature article](https://www.nature.com/articles/s41590-020-0648-y) finds that climate change can affect mosquito abundance, survival, transmission dynamics, and pathogen development within vectors.

This issue is concerning as worsening climate change trends and warmer temperatures in NYC can create more hospitable environments for mosquitoes carrying WNV and may increase human-mosquito interaction. Motivated by these concerns, we aimed to explore the rising temperatures trends in NYC, WNV incidence in NYC, and what is being done to control its spread. 

### Background and Aims
Climate change is worsening the transmission and spread of vector-borne diseases. Among these is West Nile Virus (WNV), a mosquito-borne disease that has been endemic to North America since 1999. While approximately most WNV cases are asymptomatic or lead to mild flu-like symptoms, some lead to [serious complications](https://www.cdc.gov/west-nile-virus/hcp/clinical-signs/index.html) like neuroinvasive disease which is associated with significant morbidity and mortality. 

Concerningly, WNV disease was [detected in New Yorkers for the first time in 2024](https://www.nyc.gov/site/doh/about/press/pr2024/west-nile-virus-detected-in-new-york.page). This year, there were 1,286 mosquitio pools positive for WNV across all five boroughs, which is more than double the number detected last year. An average of 17 people per year over the last decade have been diagnosed with West Nile neuroinvasive disease in NYC, with a case fatality rate of 10%. 

The potential for severe disease, long-term disability, and fatality makes WNV a serious public health concern on the horizon as temperatures continue to rise in NYC. The Heat Vulnerability Index (HVI) varies significantly across New York City neighborhoods, which could influence the incidence of mosquito vectors. 

Our project aims to uncover the relationships between climate change, location, and the incidence of West Nile Virus in New York City. Similarly to how the COVID-19 pandemic revealed stark inequities in disease morbidity and mortality by location, we would like to understand if similar patterns exist in this context.


## **Initial Questions**

With this inspiration, we wanted to explore available NYC data sets to answer some of these initial questions:

* What communities are affected most by rising temperatures?
* What do average temperatures look like over time in NYC? Is there any shift?
  * What about precipitation trends?
* If climate change is causing temperatures to rise in NYC, does this affect mosquito season?
* What is the incidence of mosquitoes carrying WNV in NYC over time? 
  * Is it different across boroughs?
* Is there a relationship between heat vulnerability index (HVI) score and incidence of WNV?
* Are the number of WNV human cases increasing over time in NYC?
  * If so, are there specific boroughs in NYC that are disproportionately affected by WNV?
* How has the NYC Department of Health and Mental Hygiene (DOHMH) mosquito control efforts during the summer affected the incidence of WNV case detection?

We hypothesize that ***between the years of 2021 to 2024, the NYC boroughs with warmer average temperatures and greater amounts of precipitation will have a higher incidence of mosquitoes carrying West Nile virus***. We also predict that areas with higher HVI scores will also have more WNV-positive mosquitoes as these areas may be more likely to experience worse heat conditions and potentially create more suitable environments for infected mosquitoes.
  
## **Data**

### Sources

We utilized the [Supplmentary Dataset of NYC Zip Codes](https://p8105.com/data/zip_codes.html) that was provided on our P8105 class website, which includes information on *NYC zip codes, boroughs*, and *neighborhoods*.

We located the [NYC Heat Vulnerability Index (HVI)](https://a816-dohbesp.nyc.gov/IndicatorPublic/data-explorer/climate/?id=2411#display=summary) through the NYC DOHMH Environment and Health Data Portal. We also utilized [Zip Code Tabulated Area Level HVI](https://catalog.data.gov/dataset/heat-vulnerability-index-rankings) data, which is designed to approximate zip codes and works well with merging HVI scores with our other datasets. The HVI shows the risk of community-level health impacts due to extreme heat and includes factors such as surface temperatures, green spaces, access to home air conditioning, median income, and the percentage of residents who are low-income or non-Latinx Black (who are often excluded from heat resources). Neighborhoods with a **HVI score of 1** have the lowest risk while those with a **HVI score of 5** have the highest risk.

Data from the [West Nile Virus Mosquito Activity reports](https://www.nyc.gov/site/doh/health/health-topics/west-nile-virus-activity.page) was used to show the detection of positive mosquitoes for West Nile virus in various NYC neighborhoods. Further, we used geographic files for [Modified ZIP Code Tabulation Areas (MODZCTA)](https://github.com/nychealth/coronavirus-data/tree/master/Geography-resources) to create the NYC maps. All data is provided by the NYC DOHMH. More information on how this mosquito surveillance is conducted by the NYC DOHMH can be found [here](https://www.nyc.gov/assets/doh/downloads/pdf/wnv/2023/wnvplan2023.pdf).

We located the [West Nile Virus NYC Human Cases](https://www.nyc.gov/assets/doh/downloads/pdf/wnv/2024/wnvplan2024.pdf) through the *2024 NYC DOHMH Comprehensive Mosquito Surveillance and Control Plan*. The report provides the number of West Nile virus human cases in NYC between 1999-2023, separated by borough.

The [Global Surface Summary of the Day (GSOD)](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00516) contains global daily average weather observations from weather stations. A `GSODR` package exists where we can load in data from specific stations and select various weather measurements. Our indicators include: mean temperature, dew point temperature, sea level pressure, station pressure, visibility, wind speed, gust, precipitation amount, and snow depth. This data is provided by the National Oceanic and Atmospheric Administration. 


### Cleaning

**P8105 Zip Codes Dataset**

The supplementary zip codes dataset was imported using `read_html` and initially cleaned using `janitor::clean_names`. We filtered the dataset to only include zip codes within NYC, used a mutate step to change borough names to: *Brooklyn*, *Manhattan*, and *Staten Island*, and only selected relevant variables: *zip_code*, *borough*, and *neighborhood*.

```{r, message=FALSE, echo=FALSE}
zip_nbhd <- read_html("https://p8105.com/data/zip_codes.html") |>
  html_table() |>
  first() |>
  janitor::clean_names() |>
  filter(!(county == "New York" & zip_code %in% c(10463, 11201))) |>#duplicated zip codes, both are *not* in New York County
  mutate(borough = str_replace_all(county, c(
    "Kings" = "Brooklyn",
    "New York" = "Manhattan",
    "Richmond" = "Staten Island"
  ))) |>
  select(zip_code, borough, neighborhood)
```

**Positive Mosquitoes Detected with West Nile Virus in NYC Datasets (2021-2024)**

For the years 2021-2023, the datasets provided by the NYC DOHMH are simple html tables that were imported using `read_html` and cleaned using `janitor::clean_names`. Zip codes for each detection of WNV-positive mosquitoes on a specific day are listed and separated by commas. To clean this, we used multiple `separate_longer_delim` steps to separate each positive mosquito detection into a separate observation, while keeping the corresponding detection date. 

For the year 2024, a .csv file was imported using `read_csv` and reshaped using `pivot_longer` to consolidate all WNV-positive mosquito detection dates into a single `date` column.

For all datasets, we reformatted the `zip_code` column to integers and the `date` column to a date object (yyyy-mm-dd), corrected any zip code and date entry errors, and removed unnecessary columns and N/A values. As a result, for each year, each row now represents a single WNV-positive mosquito detected in a specific zip code/borough/neighborhood on a specific date. 

```{r, message=FALSE, echo=FALSE, warning=FALSE}
## 2021 - 2023 are simple HTML tables
mosquitoes_2024_url <- "https://www.nyc.gov/site/doh/health/health-topics/west-nile-virus-activity-2024.page"
mosquitoes_2023_url <- "https://www.nyc.gov/site/doh/health/health-topics/west-nile-virus-activity-2023.page"
mosquitoes_2022_url <- "https://www.nyc.gov/site/doh/health/health-topics/west-nile-virus-activity-2022.page"
mosquitoes_2021_url <- "https://www.nyc.gov/site/doh/health/health-topics/west-nile-virus-activity-2021.page"

### 2024
mosquitoes_2024_table <- read_csv("data/mosquitoes_2024.csv") |>
  pivot_longer(cols = starts_with("date"), values_to = "date") |>
  select(-name, -detection_type) |>  
  mutate(
    date = case_when(
      date == "8/5/5024" ~ "8/5/2024",
      .default = date # one observation, 11426, with year miswritten as 5024-08-05 instead of 2024-08-05
    ),
    date = lubridate::mdy(date)
  ) |> 
  na.omit() 
   
### 2023
mosquitoes_2023_table <- mosquitoes_2023_url |>
  read_html() |> html_table() |> first() |>
  janitor::clean_names() |>
  separate_longer_delim(zip_code, delim = ", ") |>
  separate_longer_delim(zip_code, delim = ",\n") |>
  separate_longer_delim(zip_code, delim = ",") |> #this data is disgusting
  select(-neighborhood, -download) |>
  mutate(
    zip_code = as.integer(zip_code),
    date = str_c(date, ", ", 2023),
    date = lubridate::mdy(date)
  )

### 2022
mosquitoes_2022_table <- mosquitoes_2022_url |>
  read_html() |> html_table() |> first() |>
  janitor::clean_names() |>
  separate_longer_delim(zip_code, delim = ", ") |>
  separate_longer_delim(zip_code, delim = ",\n") |>
  separate_longer_delim(zip_code, delim = ",") |> #this data is disgusting
  select(-download) |>
  mutate(
    zip_code = as.integer(zip_code),
    date = str_c(date, ", ", 2022),
    date = lubridate::mdy(date)
  )

### 2021
mosquitoes_2021_table <- mosquitoes_2021_url |>
  read_html() |> html_table() |> first() |>
  janitor::clean_names() |>
  separate_longer_delim(zip_code, delim = ", ") |>
  separate_longer_delim(zip_code, delim = ",\n") |>
  separate_longer_delim(zip_code, delim = ",") |> #this data is disgusting
  select(-download) |>
  mutate(
    zip_code = as.integer(zip_code),
    date = str_c(date, ", ", 2021),
    date = lubridate::mdy(date)
  ) |> 
  mutate(
    zip_code = case_when(
      zip_code == 1123 ~ 11236, # data entry error on the website, where 1123 is missing the last digit - per https://www.nyc.gov/assets/doh/downloads/pdf/wnv/2021/wnv-activity-bk-08192021.pdf it should be 11236
    .default = zip_code
    )
  )
```

**NYC Positive Cases of West Nile Virus Dataset (1999-2023)**

Data was downloaded as a .csv file, imported using `read_csv`, and initially cleaned using `janitor::clean_names`. The data was reshaped using `pivot_longer` such that our columns are now: `year`, `borough`, and `wnv_cases` (i.e. the number of West Nile virus cases counted within that borough).

```{r, message=FALSE, echo=FALSE}
wnv_cases <- read_csv("data/wnv_cases.csv") |>
  janitor::clean_names() |>
  pivot_longer(cols = c(
    bronx,
    brooklyn,
    manhattan,
    queens,
    staten_island
  ),
  names_to = "borough",
  values_to = "wnv_cases") |> 
  mutate(
    borough = case_match(
      borough, 
      "manhattan" ~ "Manhattan",
      "brooklyn" ~ "Brooklyn",
      "queens" ~ "Queens",
      "bronx" ~ "Bronx",
      "staten_island" ~ "Staten Island"
    )
  )
```


**NYC Heat Vulnerability Index Dataset**

This dataset was already in a tidy format after downloading (thank you NYC DOHMH Environment and Health!!). We simply imported the dataset using `read_csv` and applied `janitor::clean_names` for consistency. Our columns include: `zip_codes` and `hvi` scores.

```{r, message=FALSE, echo=FALSE}
heat_vuln <- read_csv("data/heat_vulnerability.csv") |>
  janitor::clean_names()

hvi_zcta <- read_csv("https://data.cityofnewyork.us/resource/4mhf-duep.csv")  |> 
  rename(zip_code = zcta20)
write_csv(hvi_zcta, "cleaned_data/hvi_zcta.csv")
```


**GSODR Dataset**

Through online research, we identified Bushwick, Brooklyn as the geographical center of NYC. We loaded the `GSODR` package and used the `nearest_stations` function to identify weather stations surrounding the longitude and latitude coordinates of Bushwick. We used a 20-mile radius to capture stations across all boroughs. We then filtered out stations not in NYC and those without 2021-2024 weather data. This left us with only 5 stations: **Wall St**, **LGA**, **Central Park**, **JFK**, and **The Battery**.

We pulled weather data between 2021-2024 from the 5 weather stations using `get_GSOD`, applied `janitor::cleannames`, and selected these variables: `station name`, `latitude` and `longitude`, date (`yearmoda`) mean temperature (`temp`), mean precipitation (`prcp`), mean dew point (`dewp`) (to get an idea of water content in the air), and `max` and `min` temperature. Finally, we applied a series of mutate steps to convert temperature measurements to Fahrenheit, precipitation measures to inches, and renamed weather stations to more intuitive names.

```{r, eval=FALSE, echo=FALSE}
station_ids <-
  nearest_stations(
    LAT = 40.6958,
    LON = -73.9171,
    distance = 20
  ) |>
  filter(STATE != "NJ" & !(NAME %in% c("KINGS POINT",
                                       "SANDY HOOK",
                                       "BERGEN POINT",
                                       "MITCHEL FIELD",
                                       "HEMPSTEAD MITCHELL FLD AFB",
                                       "AMBROSE LIGHT  NY",
                                       "AMBROSE / FT. TILDEN",
                                       "FREEPORT"))) |>
  filter(!(STNID %in% c("999999-14732",
                        "725033-94728",
                        "725060-94728",
                        "999999-94728",
                        "744976-99999",
                        "999999-14786",
                        "999999-94789",
                        "725026-99999",
                        "997439-99999"))) |>
  janitor::clean_names() |>
  pull(stnid)

# weather_2021_2024 <-
#    get_GSOD(
#      years = c(2021:2024),
#      station = station_ids
#    ) |>
#    janitor::clean_names() |>
#    select(name, latitude, longitude, yearmoda, temp, max, min, prcp, dewp) |>
#    mutate(
#      temp = (temp * 9/5) + 32,
#      max = (max * 9/5) + 32,
#      min = (min * 9/5) + 32,
#      prcp = (prcp * 0.0393701),
#      year = year(yearmoda),
#      name = case_match(
#        name,
#        "PORT AUTH DOWNTN MANHATTAN WALL ST HEL" ~ "Wall St",
#        "LA GUARDIA AIRPORT" ~ "LGA",
#        "CENTRAL PARK" ~ "Central Park",
#        "JOHN F KENNEDY INTERNATIONAL AIRPORT" ~ "JFK",
#        "THE BATTERY" ~ "The Battery"
#      )
#    )

# Uncomment this line after running the previous block once
weather_2021_2024 <- read_csv("data/weather_2021_2023.csv")

#write_csv(weather_2021_2024, "data/weather_2021_2023.csv")
```


  
### Merging

To combine our datasets, we utilized the shared `zip_code` variable as a key for merging. This allowed us to successfully combine WNV-positive mosquito datasets (2021â€“2024), HVI scores, and MODZCTA shape files. For the mosquito data, we ensured that zip codes without mosquito detection were assigned a value of zero.

Our relevant variables for our datasets include:

**WNV+ Mosquito Data**

* `zip_code`: The zip code within NYC in which the WNV+ mosquitoes were detected
* `borough`: The NYC borough (Brooklyn, Manhattan, Bronx, Staten Island, or Queens) corresponding to the zip code location. 
* `date`: The date in which the WNV+ mosquito was detected within a specific zip code.
* `count`: The number of WNV+ mosquitoes detected within a specific zip code.
* `neighborhood`: The name of the NYC neighborhood in which the WNV+ mosquito was detected.
* `hvi`: The HVI score (ranging from 1-5)

```{r, message=FALSE, echo=FALSE, results='hide'}
wnv_mosquito_detection <- bind_rows(
  mosquitoes_2024_table,
  mosquitoes_2023_table,
  mosquitoes_2022_table,
  mosquitoes_2021_table,
) |> 
  left_join(y = zip_nbhd, by = join_by(zip_code)) |>
  select(zip_code, borough.x, date, neighborhood) |> 
  rename(borough = borough.x) |> # one ZIP code, 10000 has NA for borough as it is not in ZIP list - Central Park based on a Google search
  mutate(
    borough = case_when(
      zip_code == 10000 ~ "Manhattan",
      .default = borough
    ),
    neighborhood = case_when(
      zip_code == 10000 ~ "Central Park",
      .default = neighborhood
    )
  )

inner_join(wnv_mosquito_detection, hvi_zcta, by = join_by(zip_code)) |> 
  group_by(hvi) |> 
  summarize(
    count = n()
  )
```


**GSODR**

* `name`: The name of the NYC weather station
* `latitude`: Latitude coordinate of the weather station
* `longtitude`: Longitude coordinate of the weather station
* `yearmoda`: year of the weather station data that was pulled
* `temp`: mean temperature (Fahrenheit)
* `max`: maximum temperature measured (Fahrenheit)
* `min`: minimum temperature measured (Fahrenheit)
* `prcp`: mean precipitation (inches)
* `dewp`: mean dew point (Fahrenheit)

Our datasets are now ready for exploration and analysis!
  

  
  
  
  
## **Data Exploration**

***NOTE: For more information on the code we used, please visit the [Data Exploration](data_exploration.html) page on our website.*** 

### Comparing Temperature Data Across Weather Stations

We first wanted to look at which weather station to use for our temperature analysis as we were curious of whether there were any variations between temperature data from these 5 stations. We plotted the temperature data between 2021 and 2024. 

```{r, message=FALSE, echo=FALSE}
station_ids <-
  nearest_stations(
    LAT = 40.6958,
    LON = -73.9171,
    distance = 20
  ) |>
  filter(STATE != "NJ" & !(NAME %in% c("KINGS POINT",
                                       "SANDY HOOK",
                                       "BERGEN POINT",
                                       "MITCHEL FIELD",
                                       "HEMPSTEAD MITCHELL FLD AFB",
                                       "AMBROSE LIGHT  NY",
                                       "AMBROSE / FT. TILDEN",
                                       "FREEPORT"))) |>
  filter(!(STNID %in% c("999999-14732",
                        "725033-94728",
                        "725060-94728",
                        "999999-94728",
                        "744976-99999",
                        "999999-14786",
                        "999999-94789",
                        "725026-99999",
                        "997439-99999"))) |>
  janitor::clean_names() |>
  pull(stnid)

# weather_2021_2024 <-
#    get_GSOD(
#      years = c(2021:2024),
#      station = station_ids
#    ) |>
#    janitor::clean_names() |>
#    select(name, latitude, longitude, yearmoda, temp, max, min, prcp, dewp) |>
#    mutate(
#      temp = (temp * 9/5) + 32,
#      max = (max * 9/5) + 32,
#      min = (min * 9/5) + 32,
#      prcp = (prcp * 0.0393701),
#      year = year(yearmoda),
#      name = case_match(
#        name,
#        "PORT AUTH DOWNTN MANHATTAN WALL ST HEL" ~ "Wall St",
#        "LA GUARDIA AIRPORT" ~ "LGA",
#        "CENTRAL PARK" ~ "Central Park",
#        "JOHN F KENNEDY INTERNATIONAL AIRPORT" ~ "JFK",
#        "THE BATTERY" ~ "The Battery"
#      )
#    )

# Uncomment this line after running the previous block once
weather_2021_2024 <- read_csv("data/weather_2021_2023.csv")

#write_csv(weather_2021_2024, "data/weather_2021_2023.csv")

weather_2021_2024 |>
  ggplot(aes(x = yearmoda, y = temp, color = name)) +
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(se = FALSE, color = "white", alpha = 0.5) +
  theme(legend.position = "bottom") + 
  geom_errorbar(aes(ymin = min, ymax = max), alpha = 0.5)
```

We found that, despite being in different boroughs, there is not much variation in measured temperatures. Since they were all fairly similar, we ended up taking the average of the 5 weather stations. 


### Exploring Trends in WNV-Positive Mosquito Counts

We then wanted to focus on the mosquito data. Specifically, we wanted to look at the number of mosquito counts by year and borough to explore whether mosquito season starts earlier. Since we predicted that temperatures are rising in NYC, then we should expect that warmer climates may cause mosquito season to start sooner. To visualize this, we created four separate plots of counts of mosquitoes positive for WNV for each year from 2021 through 2024, separated by borough. 


```{r, message=FALSE, warning=FALSE, echo=FALSE}
mosquitoes_2021_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count, fill = borough)) +
  geom_col(stat = "identity") +
  facet_grid(~borough)

mosquitoes_2022_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count, fill = borough)) +
  geom_col(stat = "identity") +
  facet_grid(~borough)

mosquitoes_2023_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count, fill = borough)) +
  geom_col(stat = "identity") +
  facet_grid(~borough)

mosquitoes_2024_table |>
  group_by(borough, date) |> 
  summarize(
    count = n()
  ) |> 
  ggplot(aes(x = date, y = count, fill = borough)) +
  geom_col(stat = "identity") +
  facet_grid(~borough)
```

The figures show that there are drastic differences in the counts of WNV-positive mosquitoes between boroughs. Queens consistently had the highest counts of WNV-positive mosquitoes compared to the other boroughs across all years, while Manhattan had the lowest counts of WNV-positive mosquitoes. 

```{r, message=FALSE, warning=FALSE, echo=FALSE}
wnv_mosquitoes <- read_csv("cleaned_data/wnv_mosquitoes.csv")

wnv_mosquitoes |>
  mutate(
    year = lubridate::year(date),
    week = lubridate::week(date)) |>
  group_by(year, week) |>
  summarise(count = n()) |>
  mutate(weekyear = (year - 2021)*52 + week) |> 
ggplot(aes(x = weekyear, y = count)) +
geom_point()
```

Further, when boroughs are grouped together, we see that the number of WNV-positive mosquitoes peaked in 2021, followed by a decline in 2022 and 2023. There is a slight increase in the counts peak in the year 2024.

We can also see from both figures that between the years of 2021-2024, mosquito season typically ranges from June to October, which is still within a normal range of months for mosquito season in NYC. Unfortunately, the data we have access to only provides detections of WNV+ mosquitoes within NYC starting from 2021 so we won't be able to analyze long-term trends.


### Mapping WNV-Positive Mosquito Trends Across NYC

To visualize this further, we wanted to create a map of NYC between 2021-2024 and overlay the WNV positive mosquitoes data to see if we could visualize any trends on which boroughs or neighborhoods may be affected more. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# get ZIP codes that did not have counts of WNV using anti_join with original ZIP code list (for generating proportions and to ensure ZIPs without cases can still be mapped later)


all_zctas <-
  zip_nbhd |> 
  pull(zip_code) |> 
  c(10000) |> 
  expand_grid(
    zip_code = _,
    year = c(2021, 2022, 2023, 2024),
    count = 0
  )

fill_neg_zctas <- function(yr) {
  
  pos = 
    wnv_mosquito_detection |> 
    mutate(
      year = year(date)
    ) |> 
    filter(year == yr) |> 
    group_by(zip_code, neighborhood, year) |> 
    summarize(
      count = n()
    ) |>
    arrange(count)
  
  neg = 
    all_zctas |> 
    filter(year == yr) |> 
    anti_join(pos, by = join_by(zip_code))
  
  rbind(pos, neg)
    
}

wnv_mosquito_data <-
  tibble(
    years = c(2021, 2022, 2023, 2024),
    mosquito_data = map(years, fill_neg_zctas)
  ) |> 
  unnest(mosquito_data) |> 
  select(-years) |> 
  left_join(zip_nbhd, by = join_by(zip_code, neighborhood)) |> # rejoin borough variable from original ZIP code list 
  mutate(
    borough = case_when(
      zip_code == 10000 ~ "Manhattan",
      .default = borough
    ),
    neighborhood = case_when(
      zip_code == 10000 ~ "Central Park",
      .default = neighborhood
    )
  )

wnv_mosquito_data |> 
  pull(count) |> 
  sum() # 2924

wnv_mosquito_detection |> 
  nrow() # 2924 - matches

# get geographic coordinates for use in ggplot later

modzcta_path <- "data/shapefiles/MODZCTA_2010.shp"

zip_shp <-
  modzcta_path |>
  st_read() |> 
  drop_na(label) |> # gets rid of MODZCTA 99999 which is not a real MODZCTA 
  separate_longer_delim(label, delim = ", ") |> 
  janitor::clean_names() |> 
  mutate(
    modzcta = as.double(modzcta),
    zip_code = as.double(label)
  ) |> 
  separate_longer_delim(label, delim = ", ")

# get MODZCTA for each ZIP code (approximating ZIP code as ZCTA)

mos_by_modzcta <- 
  left_join(wnv_mosquito_data, zip_shp, by = join_by("zip_code")) |> 
  mutate(
    neighborhood = case_when(
      is.na(neighborhood) ~ "Non-residential",
      .default = neighborhood
    )
  )

mos_by_modzcta |>
  ggplot(aes(fill = count, geometry = geometry)) +
  geom_sf() + 
  labs(
    title = "WNV positive mosquitoes by MODZCTA",
    caption = "Data and shapefile from DOHMH"
  ) +
  theme(
    axis.text.x = element_blank(), 
    axis.title.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title.y = element_blank()
  ) +
  viridis::scale_fill_viridis(limits = c(0, 25)) +
  facet_wrap(~year)
```

Interestingly, from the figure, we found that there are zip codes in Queens and Staten Island that appear to have an increase in the number of mosquitoes positive for WNV from 2021 to 2024. Manhattan, Brooklyn, and the Bronx remained relatively low.  






## **Data Analysis**

### Data import

First, we need to import our WNV data and predictor data. Ultimately, we are only modeling WNV+ mosquitoes, so we are actually just using the mosquito data and the weather data to predict WNV+ mosquito counts. Since Heat Vulnerability Index (HVI) is a quantitative proxy for human vulnerability to extreme heat and heatstroke, we do not hypothesize it to be causally relevant. This is supported by the lack of association between HVI and WNV+ mosquito count. 

```{r data import, message=FALSE}
wnv_cases <- read_csv("cleaned_data/wnv_cases.csv")
wnv_mosquitoes <- read_csv("cleaned_data/wnv_mosquitoes.csv") 
heat_vuln <- read_csv("cleaned_data/heat_vuln.csv")
day_weather <- read_csv("cleaned_data/day_weather.csv")
```

### Model with year 

This is the simplest model, predicting counts of WNV+ mosquitoes using year. Using our mosquito count data, we want to get counts per year to model from. As such, we `mutate` to create a year variable based on the date variable, `group_by` year, and calculate the count for each year.

```{r year data}
mosq_count_by_year <- 
  wnv_mosquitoes |>
  mutate(year = lubridate::year(date)) |>
  group_by(year) |>
  summarise(count = n()) 

mosq_count_by_year |> 
  knitr::kable(col.names = c("Year", "Count of WNV+ mosquitoes")) 
```

```{r year model}
fit1 <- glm(count ~ year, mosq_count_by_year, family = "poisson")

fit1 |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Tes statistic", "p value")
  )

fit1 |> 
  glance() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Null deviance", "Null df", "Log likelihood", "AIC", "BIC", "Deviance", "Residual df", "Number of observations")
  )
```

Since we are modeling counts of WNV+ mosquitoes, viable models for count data would be a Poisson regression or negative binomial regression. Our first initial model is a Poisson regression predicting counts of mosquitoes using year alone. 

As expected, year does not really predict count much, with an estimated `r fit1 |> broom::tidy() |> filter(term == "year") |> pull(estimate) |> round(4)` count of mosquitoes for each additional year. The parameter is also not significant at the 0.05 level, with a p value of `r fit1 |> broom::tidy() |> filter(term == "year") |> pull(p.value) |> round(4)`.

### Model with month 

As for the simple model using year, we want to get counts of mosquitoes but per month this time instead of year. We create a month variable using `mutate`, and `group_by` is used again to calculate counts of mosquitoes per month.

```{r month data}

mosq_count_by_month <- 
  wnv_mosquitoes |>
  mutate(month = lubridate::month(date)) |>
  group_by(month) |>
  summarise(count = n())

mosq_count_by_month |> 
  knitr::kable(col.names = c("Month", "Count of WNV+ mosquitoes"))

mosq_count_by_month |> 
  ggplot(aes(x = month, y = count)) +
  geom_bar(stat = "identity", fill = "#440154FF") +
  guides(
    fill = "none"
  )

```

As shown on the table, the bulk of the counts are in the middle of summer in August, with `r mosq_count_by_month |> pull(count) |> max()` mosquitoes, tapering off towards the beginning and end in June and October. The start of data collection in June has the least number of WNV+ mosquitoes with `r mosq_count_by_month |> pull(count) |> min()`. When plotted, the distribution is also approximately normal.

```{r month model}
fit2 <- glm(count ~ month, mosq_count_by_month, family = "poisson") 

fit2 |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Test statistic", "p value")
  )

fit2 |> 
  glance() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Null deviance", "Null df", "Log likelihood", "AIC", "BIC", "Deviance", "Residual df", "Number of observations")
  )

```

Our second model modeling mosquito count predicted by month only has a significant regression parameter of `r fit2 |> tidy() |>  filter(term == "month") |> pull(estimate) |> round(4)` mosquitoes for every increase by 1 month, with a p value of `r fit2 |> tidy() |>  filter(term == "month") |> pull(p.value) |> round(4)`.

```{r month model fit}
mosq_count_by_month |> 
  cbind(pred = predict(fit2, type = "response")) |> 
  ggplot(aes(x = month, y = count)) +
  geom_point(color = "#404688FF", alpha = 0.5) + 
  geom_line(aes(y = pred), size = 0.6)
```

However, despite a significant model, the model does not fit our dataset well, so it is not useful in predicting counts. When plotting the predicted values against the observed values, the predicted values are linear since model is represented as a linear term, while our observed counts are parabolic.

```{r linear and quadratic month model}
fit3 <- glm(count ~ month + I(month^2), mosq_count_by_month, family = "poisson")

fit3 |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Test statistic", "p value")
  )

fit3 |> 
  glance() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Null deviance", "Null df", "Log likelihood", "AIC", "BIC", "Deviance", "Residual df", "Number of observations")
  )
```

With this model, both linear and second order terms are significant predictors of mosquito counts, with p values of 
`r fit3 |> tidy() |>  filter(term == "month") |> pull(p.value) |> round(4)` and `r fit3 |> tidy() |>  filter(term == "I(month^2)") |> pull(p.value) |> round(4)`, respectively.

```{r linear and quadratic month fit}
mosq_count_by_month |> 
  cbind(pred = predict(fit3, type = "response")) |> 
  ggplot(aes(x = month, y = count)) +
  geom_point(alpha = 0.5, size = 2) + 
  geom_line(aes(y = pred), color = "#3B528BFF", size = 0.6)

```

With an included second order term for month, our predicted values better fit the observed values, depicting the parabolic shape of the data. 

### Model with month and year

Now that we are predicting both month and year, we want to get counts per each combination of month and year. We `mutate` to create month and year variables again, `group_by` month and year both this time, and get the mosquito counts per each combination as follows. 

```{r month year data, message=FALSE}
mosq_month_year <- 
  wnv_mosquitoes |>
  mutate(month = lubridate::month(date),
         year = lubridate::year(date)) |>
  group_by(year, month) |>
  summarise(count = n()) 

mosq_month_year |> 
  knitr::kable(col.names = c("Year", "Month", "Count of WNV+ mosquitoes"))
```

To determine whether year is a significant additional predictor to a model that already has linear and second order terms for month, we created a Poisson model with just the two month terms and one with both month terms and year as a predictor.

```{r month year model}
fit3.5 <- glm(count ~ month + I(month^2), mosq_month_year, family = "poisson")
fit4 <- glm(count ~ month + I(month^2) + year, mosq_month_year, family = "poisson")

fit3.5 |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Test statistic", "p value")
  )

fit3.5 |> 
  glance() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Null deviance", "Null df", "Log likelihood", "AIC", "BIC", "Deviance", "Residual df", "Number of observations")
  )

fit4 |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Test statistic", "p value")
  )

fit4 |> 
  glance() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Null deviance", "Null df", "Log likelihood", "AIC", "BIC", "Deviance", "Residual df", "Number of observations")
  )

anova(fit3.5, fit4) |> 
  tidy()

```

In the model with year as a predictor, year is not significantly predicting mosquito counts, with a p value of `r fit4 |> tidy() |> filter(term == "year") |> pull(p.value) |> round(4)`. When conducting a chi-squared test to determine whether the addition of year is significant, the associated p value is `r anova(fit3.5, fit4) |> tidy() |> filter(term == "count ~ month + I(month^2) + year") |> pull(p.value) |> round(4)`. Adding year to a model that already has month as linear and second order terms does not significantly improve model prediction. 

```{r poisson overdispersion}
mosq_month_year |> 
  cbind(pred = predict(fit4, type = "response")) |> 
  mutate(monthyear = (year - 2021)*12 + month) |> 
  ggplot(aes(x = monthyear, y = count)) +
  geom_point(alpha = 0.5) +
  geom_line(aes(y = pred, group = year), color = "#365D8DFF", size = 0.6)

mosq_month_year |> 
  cbind(
    pred = predict(fit4, type = "response"),
    resid = resid(fit4)
  ) |> 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(color = "#31688EFF",, size = 3) + 
  geom_hline(yintercept = 0)

performance::check_overdispersion(fit4) 


```

However, before proceeding with the next model, we wanted to check model fit and see if the assumptions of the Poisson distribution still hold. When graphing the predicted values for the model with month and year, it is able to capture the peaks and declines in counts better than the simple linear model, but isn't quite the best fit. 

One of the Poisson model assumptions is that variance is approximately equal to the mean, so as the mean increases, the variance does as well. However, when the observed variance is much higher than the mean, and therefore higher than the expected variance, the data is overdispersed and a Poisson model may not be the best choice for the data. 

When plotting the residuals vs fitted values, we would expect a fanning pattern as we would see the variance increase as the mean predicted values increase. However, at higher predicted values, the variance starts to decrease, indicating overdispersion. 

This is confirmed using the `check_overdispersion` test from the `performance` package, which tests the dispersion ratio of observed variance to the expected variance. The null hypothesis of this test is that the dispersion ratio is 1, so the observed and expected variances are equal. However, our test result is significant, rejecting the null hypothesis and indicating our model is overdispersed for our data. This suggests we may need to use a different model that accounts for dispersion, like a negative binomial model. 



### Model with week and year

To create a model that includes week as a parameter, we need aggregated mosquito counts by year and week. This is done again by `mutate` to make year and week variables, creating a variable `weekyear` that calculates the number of weeks from 2021 (the first year of available mosquito data), and grouping counts by combinations of year and week. This `weekyear` variable is used to merge with weather data from GSOD. 

Daily weather readings from GSOD are aggregated to be by week, calculating the average temperature, precipitation, dewpoint, maximum temperature, and minimum temperature across several stations in NYC, by combinations of year and week. Additionally, since week-to-week weather is likely not independent as the weather conditions of the prior week probably influences the following week, lagging temperature variables are also created for the above weather variables. `weekyear` was also created. 

The mosquito counts and weather data are merged by `weekyear`, with duplicate columns from the weather data dropped. As with the weather, circulating WNV+ mosquitoes from week to week are also hypothesized to not be independent, since the presence of WNV+ circulating the week prior will persist and infect other mosquitoes in the following week. Therefore, lagging mosquito counts are also created.

```{r week and temp data}

# mosq_cases |> 
#   ggplot(aes(x = weekyear, y = count)) +
#   geom_point()
mosq_cases <-
  wnv_mosquitoes |>
  mutate(
    year = lubridate::year(date),
    week = lubridate::week(date)) |>
  group_by(year, week) |>
  summarise(count = n()) |>
  mutate(weekyear = (year - 2021)*52 + week) 
  
weekly_weather <- 
  day_weather |>
  mutate(
    year = lubridate::year(date),
    week = lubridate::week(date)) |>
  group_by(year, week) |>
  summarise(avg_temp = mean(avg_temp),
            avg_prcp = mean(avg_prcp),
            avg_dewp = mean(avg_dewp),
            avg_max = mean(avg_max),
            avg_min = mean(avg_min)) |>
  arrange(year, week) |>
  mutate(lag_temp = lag(avg_temp),
         lag_prcp = lag(avg_prcp),
         lag_dewp = lag(avg_dewp),
         lag_max =  lag(avg_max),
         lag_min =  lag(avg_min)) |>
  mutate(weekyear = (year - 2021)*52 + week) #week since start of 2021

mosq_cases_temp <- 
  mosq_cases |>
  left_join(y = weekly_weather, by = join_by(weekyear)) |>
  select(-ends_with(".y")) |>
  rename(year = year.x, week = week.x) |>
  mutate(
    id = row_number(),
    lag_count = lag(count),
    lag_count = case_when(
      is.na(lag_count) ~ 0,
      .default = lag_count
    )
  ) |>
  na.omit()

mosq_cases_temp |> 
  knitr::kable()
```

The first model is just the week as a linear and a second order term as well as year to predict mosquito counts, using a negative binomial model this time.

```{r week year model}
null_fit <- MASS::glm.nb(count ~ week + I(week^2) + year, mosq_cases_temp)

null_fit |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Test statistic", "p value")
  )

null_fit |> 
  glance() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Null deviance", "Null df", "Log likelihood", "AIC", "BIC", "Deviance", "Residual df", "Number of observations")
  )

mosq_cases_temp |> 
  cbind(pred = predict(null_fit, type = "response")) |> 
  ggplot(aes(x = weekyear, y = count)) +
  geom_point(alpha = 0.5, size = 2) + 
  geom_line(aes(y = pred, group = year), color = "#472D7BFF", size = 0.6)

mosq_cases_temp |> 
  cbind(resid = resid(null_fit)) |> 
  ggplot(aes(x = count, y = resid)) +
  geom_point(color = "#472D7BFF") # null model works well
```

All predictors in the model are significant, with p values for first order week and second order week of `r null_fit |> tidy() |> filter(term == "week") |>  pull(p.value) |> round(4)` and `r null_fit |> tidy() |> filter(term == "I(week^2)") |>  pull(p.value) |> round(4)`, respectively. Year has a p value of `r null_fit |> tidy() |> filter(term == "year") |>  pull(p.value) |> round(4)`. 

When fitting the model to the data, we see that it also fits well.

### Fitting temperature

Before we continue, we note another regression assumption may be violated. A standard generalized linear model assumes that the samples are independent. In most situations this assumption is trivial, but in our case it becomes an issue. Since mosquitoes reproduce, mosquito incidence in a sufficiently short time period is highly dependent on the incidence in a previous time period. The null model is essentially guessing this effect based on the average in the other years. This leads to an inappropriate fit for the data in 2021. To address this problem, we fit an autoregressive model based on the previous week.

```{r lag count model}
ar_fit <- MASS::glm.nb(count ~ lag_count, mosq_cases_temp)

ar_fit |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Test statistic", "p value")
  )

ar_fit |> 
  glance() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Null deviance", "Null df", "Log likelihood", "AIC", "BIC", "Deviance", "Residual df", "Number of observations")
  )

mosq_cases_temp |> 
  cbind(pred = predict(ar_fit, type = "response")) |> 
  ggplot(aes(x = weekyear, y = count)) +
  geom_point(alpha = 0.5, size = 2) + 
  geom_line(aes(y = pred, group = year), color = "#31688EFF", size = 0.6)

mosq_cases_temp |> 
  cbind(resid = resid(ar_fit)) |> 
  ggplot(aes(x = count, y = resid)) +
  geom_point(color = "#31688EFF") 
```

Now, we have a model with just the lagging count of mosquitoes. The estimated increase in counts based on the prior count of mosquitoes is `r ar_fit |> tidy() |> filter(term == "lag_count") |> pull(estimate) |> round(4)` with a p value of `r ar_fit |> tidy() |> filter(term == "lag_count") |> pull(p.value) |> round(4)`. The lagging mosquito count is then a significant predictor of estimated mosquito counts, and the graph illustrates the model fits the observed points well, except for the peaks in the 2021 data.

```{r lag count temp model}
ar_temp_fit <- MASS::glm.nb(count ~ lag_count + lag_temp, mosq_cases_temp)

ar_temp_fit |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Test statistic", "p value")
  )

ar_temp_fit |> 
  glance() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Null deviance", "Null df", "Log likelihood", "AIC", "BIC", "Deviance", "Residual df", "Number of observations")
  )

mosq_cases_temp |> 
  cbind(
    pred_ar = predict(ar_fit, type = "response"),
    pred_ar_temp = predict(ar_temp_fit, type = "response")
  ) |> 
  ggplot(aes(x = weekyear, y = count)) +
  geom_point(alpha = 0.5, size = 2) + 
  geom_line(aes(y = pred_ar, color = "AR", group = year), linetype = "dashed", size = 0.6) +
  geom_line(aes(y = pred_ar_temp, color = "AR+temp", group = year), linetype = "solid", size = 0.6) +
  scale_color_manual(name = "models", values = c("AR" = "#31688EFF", "AR+temp" = "#1F9A8AFF"))

mosq_cases_temp |> 
  cbind(resid = resid(ar_temp_fit)) |> 
  ggplot(aes(x = count, y = resid)) +
  geom_point(color = "#1F9A8AFF") 

```

Afterwards, a model that also has lagging average temperature has significant terms for both lagging temperature and lagging mosquito counts. The associated estimate for lagging count is now `r ar_temp_fit |> tidy() |> filter(term == "lag_count") |> pull(estimate) |> round(4)`, with a p value of `r ar_temp_fit |> tidy() |> filter(term == "lag_count") |> pull(p.value) |> round(4)`. For lagging temperature, the estimate is `r ar_temp_fit |> tidy() |> filter(term == "lag_temp") |> pull(estimate) |> round(4)` , with a  p value of `r ar_temp_fit |> tidy() |> filter(term == "lag_temp") |> pull(p.value) |> round(4)`. This model indicates lagging temperature has a larger effect on estimating mosquito counts than lagging mosquito counts. Compared to the model with just lagging mosquito counts, lagging mosquito counts has a slightly smaller effect on predicting counts in this model. Note the addition of the temperature variable improves the prediction notably for 2021.

Lastly, when we explored Poisson models, we had a saturated model that included precipitation as a predictor and found it was a considerably significant predictor with a larger effect size than the other predictors. However, since a Poisson regression is overdispersed for our data, we wanted to see if precipitation also has a considerable impact on predicting mosquito counts in a negative binomial regression model. 

```{r lag count temp rain}
ar_temp_rain_fit <- MASS::glm.nb(count ~ lag_count + lag_temp + lag_prcp, mosq_cases_temp)

ar_temp_rain_fit |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Test statistic", "p value")
  )

ar_temp_rain_fit |> 
  glance() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Null deviance", "Null df", "Log likelihood", "AIC", "BIC", "Deviance", "Residual df", "Number of observations")
  )

mosq_cases_temp |> 
  cbind(
    pred_null = predict(null_fit, type = "response"),
    pred_ar_temp = predict(ar_temp_fit, type = "response"),
    pred_ar_temp_rain = predict(ar_temp_rain_fit, type = "response")
  ) |> 
  ggplot(aes(x = weekyear, y = count)) +
  geom_point(alpha = 0.5, size = 3) + 
  geom_line(aes(y = pred_null, color = "null", group = year), linetype = "dotted", size = 1.9) +
  geom_line(aes(y = pred_ar_temp, color = "AR+temp", group = year), linetype = "solid", size = 0.6) +
  geom_line(aes(y = pred_ar_temp_rain, color = "AR+temp+rain", group = year), linetype = "dashed", size = 0.6) +
  scale_color_manual(name = "models", values = c("null" = "#472D7BFF", "AR+temp" = "#1F9A8AFF", "AR+temp+rain" = "#5DC863FF"))

mosq_cases_temp |> 
  cbind(resid = resid(ar_temp_rain_fit)) |> 
  ggplot(aes(x = count, y = resid)) +
  geom_point(color = "#5DC863FF") 
```

For this model, the estimated coefficient for lagging mosquito counts is `r ar_temp_rain_fit |> tidy() |> filter(term == "lag_count") |> pull(estimate) |> round(4)`, compared to its coefficient in the model without rain with `r ar_temp_fit |> tidy() |> filter(term == "lag_count") |> pull(estimate) |> round(4)`. The p value for this estimate is `r ar_temp_rain_fit |> tidy() |> filter(term == "lag_count") |> pull(p.value) |> round(4)`, while it has a p value in the prior model of `r ar_temp_rain_fit |> tidy() |> filter(term == "lag_count") |> pull(p.value) |> round(4)`. With lagging average temperature, the estimate is now `r ar_temp_rain_fit |> tidy() |> filter(term == "lag_temp") |> pull(estimate) |> round(4)` (p = `r ar_temp_rain_fit |> tidy() |> filter(term == "lag_temp") |> pull(p.value) |> round(4)` compared to `r ar_temp_fit |> tidy() |> filter(term == "lag_temp") |> pull(estimate) |> round(4)` (p = `r ar_temp_fit |> tidy() |> filter(term == "lag_count") |> pull(p.value) |> round(4)`). 

The new term of lagging precipitation has an estimate of `r ar_temp_rain_fit |> tidy() |> filter(term == "lag_prcp") |> pull(estimate) |> round(4)` with a p value of `r ar_temp_rain_fit |> tidy() |> filter(term == "lag_prcp") |> pull(p.value) |> round(4)`. Despite having a large effect size compared to the other predictors, it is now no longer a significant estimate in a negative binomial model. This is likely from adjusting for the overdispersion, which would have biased the standard error of the lagging precipitation estimate towards zero and artificially inflated the significance. In terms of model fit, including the lagging precipitation term only slightly improved model fit for 2023. As such, for maintaining parsimony, our final model only includes lagging average temperature and lagging mosquito counts, not lagging rain.


```{r final model}
# rain doesnt significantly contribute, but makes fit visually better
# especially for 2023

final_fit <- ar_temp_fit

final_fit |> 
  ggcoef_model(
    variable_labels = c(
      lag_count = "Lagging \nmosquito count",
      lag_temp = "Lagging \naverage temperature"
    ),
    signif_stars = FALSE
    ) +
    labs(
      title = "Predicted WNV+ mosquito counts in NYC",
      x = "Negative binomial regression estimated coefficients"
    )

```

```{r influence analysis}
augment_df <- 
  augment(final_fit) |>
  mutate(
    id = row_number(),
    avg_hat_2 = 2*mean(.hat),
    avg_hat_3 = 3*mean(.hat),
    influential = case_when(
      .cooksd > avg_hat_2 ~ 1,
      .cooksd > avg_hat_3 ~ 1,
      .std.resid > 2 ~ 1, 
      .default = 0
    )
  ) |> 
  select(id, everything())

avg_hat_2 <- 
  augment_df |> 
  pull(avg_hat_2) |> 
  nth(1)
  
avg_hat_3 <-
  augment_df |> 
  pull(avg_hat_3) |> 
  nth(1)
  
augment_df |> 
  ggplot(aes(x = .hat, y = .std.resid)) + 
  geom_point(aes(size = .cooksd, color = .cooksd)) +
  geom_hline(
    yintercept = c(-2, 0, 2)
  ) +
  geom_vline(
    xintercept = c(avg_hat_2, avg_hat_3)
  ) +
  guides(
    color = "none",
    size = "none"
  )

augment_df |> 
  filter(influential == 1) |> 
  select(-c(id, starts_with("avg_hat"), influential)) |> 
  knitr::kable(
    digits = 4,
    col.names = c("Count", "Lagging count", "Lagging temperature", "Fitted", "Residual", "Hat", "Sigma", "Cook's distance", "Studentized residuals")
  )

# car::influencePlot(final_fit) |> 
#  knitr::kable(col.names = c("Studentized residuals", "Hat values (influence)", "Cook's distance"))
```

When looking at influential points in our dataset, there are 3 different influential points that may warrant further analysis to see if omitting them alter the model parameter estimates.

```{r omit influential points}

influential_ids <- 
  augment_df |> 
  filter(influential == 1) |> 
  pull(id)

final_fit_omitted <-
  mosq_cases_temp |> 
  select(id, count, lag_count, everything()) |> 
  filter(!(id %in% influential_ids)) |> 
  MASS::glm.nb(count ~ lag_count + lag_temp, data = _)
  
final_fit |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Test statistic", "p value")
  )

final_fit |> 
  ggcoef_model(
    variable_labels = c(
      lag_count = "Lagging \nmosquito count",
      lag_temp = "Lagging \naverage temperature"
    ),
    signif_stars = FALSE
    ) +
    labs(
      title = "Predicted WNV+ mosquito counts in NYC",
      x = "Negative binomial regression estimated coefficients"
    )

final_fit_omitted |> 
  tidy() |> 
  knitr::kable(
    digits = 4,
    col.names = c("Term", "Estimate", "Standard error", "Test statistic", "p value")
  ) 

final_fit_omitted |> 
  ggcoef_model(
    variable_labels = c(
      lag_count = "Lagging \nmosquito count",
      lag_temp = "Lagging \naverage temperature"
    ),
    signif_stars = FALSE
    ) +
    labs(
      title = "Predicted WNV+ mosquito counts in NYC",
      x = "Negative binomial regression estimated coefficients"
    )


```

When omitting the influential points, the lagging count estimate does not change much, going from `r final_fit |> tidy() |> filter(term == "lag_count") |> pull(estimate) |> round(4)` to `r final_fit_omitted |> tidy() |> filter(term == "lag_count") |> pull(estimate) |> round(4)`. The lagging temperature estimate does change a bit, changing from `r final_fit |> tidy() |> filter(term == "lag_temp") |> pull(estimate) |> round(4)` to `r final_fit_omitted |> tidy() |> filter(term == "lag_count") |> pull(estimate) |> round(4)`.

```{r predicted vals}
pred_vals_omit <-
  mosq_cases_temp |> 
  filter(!(id %in% influential_ids)) |>  
  cbind( 
    pred_fin_omit = predict(final_fit_omitted, type = "response")
  ) |> 
  select(id, count, pred_fin_omit)

pred_vals <-
  mosq_cases_temp |> 
  cbind(
    pred_fin = predict(final_fit, type = "response")
  ) |> 
  select(id, count, pred_fin)

inner_join(pred_vals, pred_vals_omit, by = join_by(id)) |> 
  select(-ends_with(".y")) |>
  rename(count = count.x) |> 
  ggplot(aes(x = count, y = pred_fin)) +
  geom_point(color = "#404788FF") +
  geom_point(aes(y = pred_fin_omit), color = "#55C667FF") +
  labs(
    y = "Predicted values",
    x = "Observed values"
  )
```
When plotting predicted values from both models for a given observed count, the values do not seem to differ much either, especially for smaller observed values. These influential points therefore do not seem to be weighing the model heavily. 
  
  

## Discussion

### Highlights
* WNV(+) mosquito counts follow a distinct seasonal pattern (large increases July to September, peak in August)  
* Incidence of WNV(+) mosquitos differs significantly by borough, but average temperature does not  
* Higher temperatures associated with WNV(+) mosquito incidence  
* WNV(+) mosquito count is associated with week & month of year, but not year (incidence did not increase year-to-year)  
* Lagging analysis: prior temperatures and WNV(+) mosquito counts predict current WNV(+) mosquito counts, but precipitation does not  

### Results
Data Exploration: 
Analysis of Global Surface Summary of the Day (GSODR) data showed that variation in average temperature between boroughs was minor. This supported the decision to average temperature data across five weather stations. 

From 2021 to 2024, the distribution of WNV(+) mosquitoes differs by borough. Queens consistently has the highest counts, while counts were lowest in Manhattan and Staten Island. 



#### Temporal Trends in WNV(+) Mosquito Counts
We initially expected that with time, WNV(+) mosquitos detected would increase. 
On the contrary, we did not see an association between WNV(+) mosquito count when just modelling by year (estimate = -0.0271, p=0.1016). 

When combining WNV(+) mosquito count data by month for all four years (2021-2024), we see a distinct mosquito season arise. We see a large jump in cases from July to September, with counts being the highest in August. 

Month is significantly associated with WNV(+) mosquito count (estimate = 0.04, p = 0.0022), but these findings are not entirely useful because our data does not appear to fit a linear model well. When including a month-squared term in the model, we see a better fit of the model, suggesting that mosquito counts follow a parabolic shape of the data. 

Our data was overdispersed (variance greater than mean), which violates a key assumption of the Poisson model. With this, a Negative Binomial model was made to assess the effects of time. In addition, using time as a predictor masked that the samples per sufficiently short time period were technically not independent.

When conducting a similar model as above with first and second-order terms for week-of-the-year and year using Negative Binomial regression, we saw significant results. The NB model including week-of-the-year, week-of-the-year squared, and year all had significant p-values (less than 0.05). 
These results demonstrate that WNV(+) mosquito counts exhibit a seasonal trend that appears to follow a curved or parabolic pattern across both weeks and months of the year, supported by the improved model fit with the inclusion of second-order terms.

#### Lagging Analysis: Past Count, Temperature, and Precipitation
We performed a lagging analysis to identify the impact of past mosquito count, temperature, and precipitation on WNV(+) mosquito counts. This is based on the assumption that mosquito counts, temperature, or precipitation from prior days can impact mosquito counts in the present.

Mosquito incidence is highly dependent on incidence in previous time periods due to the short reproductive cycle of mosquitoes. Past temperature and precipitation go hand-in-hand, and can directly contribute to future mosquito counts because rainfall creates breeding and larval development sites. While lagging precipitation was significant in the Poisson model, this was likely due to the effects of our overdispersed data. Since lagging precipitation did not improve model fit overall, the final model includes only lagging average temperature and lagging mosquito counts. 

The mosquito season in 2021 had unusually high levels of WNV(+) mosquitoes detected. These outliers caused a slight positive bias in the predictions for 2022 and 2023, but did not affect significance of any variable in our dataset. Instead of omitting the outliers, we added a quadratic term for lagged counts which improved the predictive accuracy for all years except 2021. 

Using a Negative Binomial model with a linear effect of the previous weekâ€™s mean temperature and a quadratic effect of the previous weekâ€™s count, estimated increase in WNV(+) mosquito count was significantly associated with prior mosquito counts (linear estimate =0.0430, p < 1e-4, quadratic effect = -2e-4, p < 1e-4) and prior temperature (estimate =0.0717, p < 1e-4).

We ultimately cannot conclude that the presence of West Nile Virus positive mosquitoes has increased per year in the timespan that our data spans, but the significant positive coefficient of temperature in our model implies that higher temperatures will lead to a larger WNV(+) mosquito incidence.


### Limitations
A major limitation of our analysis is that mosquito count data was only available for four years. This made it very difficult to assess temporal trends in mosquito activity over the years, which is critical for understanding the effects of climate change on mosquito incidence. 

Furthermore, our project did not include an analysis of human WNV cases in NYC due to the limited availability of data. There was no geographic (ZIP code) information on where human cases came from, so these data did not add value to our analysis. Without this, we are unable to assess the impact of WNV(+) mosquito incidence on the number of human cases of WNV, which was the initial question we wanted to answer.


### Closing
Ultimately, our analysis showed that year-to-year climate change may not yet be having a major impact on WNV(+) mosquito incidence in NYC, and rather that we should focus on temporal trends during peak mosquito reproductive months to tailor mosquito control strategies. 

West Nile Virus may not be as big of a public health issue in New York City as we initially expected, but our findings about temperature serve as a warning for the future as global warming progresses. Future research on the relationships between mosquito incidence and human cases is necessary to truly understand the severity of the issue. 






